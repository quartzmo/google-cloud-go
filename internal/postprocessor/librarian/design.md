# Go Librarian Migration

# Objective

To onboard Go GAPIC client library generation to the Librarian 0.2 platform, replacing the legacy `bazel-bot` and `OwlBot` toolchain with a modern, container-based solution.

The primary goal is to achieve a **backward-compatible migration**. The assets generated by the new system must be identical to those produced by the existing process, including generated source code and the externally-located snippets files. This ensures a seamless transition with no impact on users or downstream release tooling.

The initial scope of this effort is to migrate a single, representative client library (e.g., Google Cloud Workflows) to validate the process before proceeding with a full-scale migration.

# Background

The current code generation process is a complex, multi-stage pipeline distributed across several tools and repositories:

1.  **Generation:** `bazel-bot` invokes Bazel to generate Go source code from the public API definitions in `googleapis` into a private intermediate repository, `googleapis-gen`.
2.  **File Placement:** `OwlBot` copies the generated code from `googleapis-gen` into the final `google-cloud-go` repository, applying a series of complex regex rules for file inclusion and preservation defined in a central `.OwlBot.yaml` file.
3.  **Post-Processing:** Finally, a Go-specific `OwlBot` post-processor runs to perform cleanup, apply code modifications, and generate additional files (like `version.go`) that are not handled by the core `protoc` generators.

This architecture is vulnerable to security and maintenance concerns, as documented in the [1PP Engineering Plan](https://docs.google.com/document/d/1_iLARXR7ZEJALSyXUjdFWhFbSJq9GCDhu1BUUIPKLkI/edit). The Librarian project was created to replace this legacy toolchain with a standardized, secure, and maintainable system.

# Overview

This document outlines a phased migration to a containerized Go generator that conforms to the [Librarian 0.2 contract](http://go/librarian:cli-reimagined). The core strategy is to adapt existing tools and inputs to the new workflow, deferring major architectural changes (like replacing Bazel configuration files) to accelerate delivery and reduce risk.

The new workflow will be orchestrated by the central Librarian tool and will consist of three main stages, each corresponding to a command passed to the Go `librariangen` container:

1.  **`configure` (Onboarding):** For a new library, Librarian invokes the `librariangen` container to enrich a minimal request with Go-specific conventions (e.g., deriving module paths) and returns a full configuration to be stored in the repository's central `state.yaml`.
2.  **`generate` (Code Generation):** Librarian invokes the `librariangen` container to perform the core code generation. The container receives API definitions, invokes `protoc`, runs a custom Go post-processor, and writes the final, backward-compatible code to an `/output` directory.
3.  **`build` (Validation):** Librarian copies the generated code into a full checkout of the `google-cloud-go` repository and invokes the `librariangen` container to run `go build` and `go test`, validating that the new code integrates correctly.

The high-level workflow for the `generate` step is as follows:

1.  **Invocation:** The Librarian tool invokes the Go `librariangen` Docker container with a specific command (e.g., `generate`).
2.  **Inputs:** Librarian provides all necessary inputs as mounted directories:
    *   `/source`: A complete checkout of the `googleapis` repository.
    *   `/librarian`: Contains a `generate-request.json` file specifying which API to generate.
    *   `/output`: An empty directory for the generated files.
    *   `/input`: A directory for optional, user-provided templates and scripts.
3.  **Execution:** The Go `librariangen` binary runs inside the container.
    *   It parses the `generate-request.json` to determine the target API.
    *   It invokes `protoc` with the Go plugins (`protoc-gen-go`, `protoc-gen-go_gapic`), using `/source` as the single import path (`-I`).
    *   It writes all generated `.go` files directly to the `/output` directory.
4.  **Post-Processing:** After generation, a built-in post-processing step formats the code, generates a `version.go` file, and runs linters.
5.  **Output:** The Librarian tool takes the contents of the `/output` directory and copies them to the correct location in the target `google-cloud-go` repository.

This approach encapsulates the entire generation process within a single, well-defined container, eliminating dependencies on the legacy `bazel-bot` and `OwlBot` infrastructure.

# Detailed Design

The implementation consists of a Go application packaged into a Docker container. The application's entrypoint dispatches to different logic based on the command-line argument provided by Librarian.

### **Container (`librariangen/Dockerfile`)**

The `librariangen` Docker container is built using a MOSS-compliant `debian:12` base image. The Dockerfile is responsible for:
*   Installing specific, pinned versions of Go (`1.23.0`), `protoc` (`25.7`), and other required tools.
*   Installing the necessary Go protoc plugins: `google.golang.org/protobuf/cmd/protoc-gen-go` and `github.com/googleapis/gapic-generator-go/cmd/protoc-gen-go_gapic`.
*   Copying the `librariangen`'s Go source code into the image and building it into a single executable binary (`/librariangen`).
*   Setting the `ENTRYPOINT` to this `/librariangen` binary.

### **Librarian Go Generator Binary (`librariangen/main.go`)**

The Go application serves as the container's entrypoint. It is a simple command-line application that dispatches logic based on the first argument passed to the container.

#### **`generate` Command**
This is the core command, responsible for the main generation logic.

##### **Inputs:**
    *   `/librarian/generate-request.json`: A JSON file describing the library to generate.
    *   `/source`: A full checkout of the `googleapis` repository.
    *   `/input`: Optional user-provided templates or scripts.

##### **Execution:**

1.  **Parse Request:** Read and unmarshal the `generate-request.json`. This can range from a simple, single-API client to a complex one with multiple versions and nested API paths.

    *Example `generate-request.json` (Complex: Workflows):*
    ```json
    {
      "id": "google-cloud-workflows",
      "apis": [
        { "path": "google/cloud/workflows/v1", "service_config": "workflows_v1.yaml" },
        { "path": "google/cloud/workflows/v1beta", "service_config": "workflows_v1beta.yaml" },
        { "path": "google/cloud/workflows/executions/v1", "service_config": "workflowexecutions_v1.yaml" },
        { "path": "google/cloud/workflows/executions/v1beta", "service_config": "workflowexecutions_v1beta.yaml" }
      ]
    }
    ```
2.  **Load Configuration:** For each API, it parses the corresponding `BUILD.bazel` file located in the `/source` directory (e.g., `/source/google/cloud/workflows/v1/BUILD.bazel`). This file is the source of truth for `protoc` options such as the transport layer (`grpc+rest`), service YAML path, and release level. This avoids having to duplicate this configuration.
3.  **Execute `protoc`:** It constructs and executes the `protoc` command. See [gapicgen](TODO #gapicgen) below for more details.
4.  **Post-Process:** Run the adapted google-cloud-go post-processor on the contents of `/output` to ensure backward compatibility. This includes formatting, linting, generating `version.go`, and updating files. See [postprocessor](TODO #postprocessor) below for more details.

##### **Output:**
    *   `/output`: A directory containing the complete, formatted, and backward-compatible Go client library, matching the structure of the legacy system.

*Example expected output structure for `google-cloud-workflows`:*
```shell

└── google-cloud-go
    ├── workflows
    │   └── apiv1
    │       ├── workflows_client.go
    │       ├── workflowspb
    │       │   ├── workflows_service.pb.go
    │       │   └── ...
    │       ├── doc.go
    │       └── ...
    └── internal
        └── generated
            └── snippets
                └── workflows
                    └── apiv1
                        ├── Client
                        │   ├── AnalyzeIamPolicy
                        │   │   └── main.go
                        │   └── ...
                        └── snippet_metadata.google.cloud.workflows.v1.json
```

#### gapicgen

The paths may include nested APIs as shown in the `workflows` example below:

##### generate-request.json

```json
{
  "id": "google-cloud-workflows",
  "apis": [
    {
      "path": "google/cloud/workflows/v1",
      "service_config": "workflows_v1.yaml"
    },
    {
      "path": "google/cloud/workflows/v1beta",
      "service_config": "workflows_v1beta.yaml"
    },
    {
      "path": "google/cloud/workflows/executions/v1",
      "service_config": "workflowexecutions_v1.yaml"
    },
    {
      "path": "google/cloud/workflows/executions/v1beta",
      "service_config": "workflowexecutions_v1beta.yaml"
    }
  ]
}

```

See the complete set of protos and related BUILD.bazel configuration for the Workflows API at [googleapis/google/cloud/workflows](https://github.com/googleapis/googleapis/tree/master/google/cloud/workflows/).

The key arguments to `protoc`are:
*   `--go_out=/output` and `--go-gapic_out=/output`: Directs all generated files to the output directory.
*   `-I=/source`: Sets the `googleapis` checkout as the sole import path, simplifying dependency resolution.
*   `--go-gapic_opt=...`: Passes the options extracted from the `BUILD.bazel` file to the [Go GAPIC Generator](TODO).

#### postprocessor

As you can see from the directory structure above, some client library files must be written outside of the module's source tree (the `google-cloud-go/workflows` directory). In fact, there are a number of locations in the `google-cloud-go` monorepo that reference client library modules and must potentially be updated. They are:

*   `.github/.OwlBot.yaml` (manual edit)
*   `internal/generated/snippets/<module>/<version>/**/*`
*   `internal/generated/snippets/go.mod`
*   `internal/postprocessor/config.yaml` (manual edit)
*   `internal/.repo-metadata-full.json`
*   `.release-please-manifest-submodules.json`
*   `go.work`
*   `release-please-config-yoshi-submodules.json`

Shared files NOT modified by the current post-processor:

*   `.github/.OwlBot.yaml`: This is a manual edit. The post-processor does not modify it.
*   `internal/postprocessor/config.yaml`: This is a manual edit. The post-processor does not modify it.

Module-scope (not shared) files modified by the current post-processor:

*   `internal/generated/snippets/<module>/<version>/**/*`: `UpdateSnippetsMetadata` in `main.go` updates a $VERSION placeholder within the `snippet_metadata.google.cloud.*.json` files within the `<module>/<version>` directories.

Shared files modified by the current post-processor:

*   `internal/.repo-metadata-full.json`: `Manifest` in `manifest.go` creates or truncates the file and then writes all modules' metadata to it.
*   `internal/generated/snippets/go.mod`: `modEditReplaceInSnippets` in `main.go` runs a command to update this file when new modules are added.
*   `.release-please-manifest-submodules.json`: `UpdateReleaseFiles` in `releaseplease.go` adds new modules to this manifest with a starting version of `0.0.0`.
*   `go.work`: `generateModule` in `main.go` calls a command to add new modules to the `go.work` file.
*   `release-please-config-yoshi-submodules.json`: `UpdateReleaseFiles` in `releaseplease.go` regenerates this file to include all modules.

See [Risks - Updating Shared Files](TODO) below for more details.

#### **Post-Processing**

A critical part of this design is adapting the logic from the existing `OwlBot` post-processor and `.OwlBot.yaml` configuration into the new containerized workflow.

*   **Handwritten File Preservation:** The process must not overwrite essential handwritten files. A list of preservation rules (e.g., `packages/<package>/tests/system/**`, `CHANGELOG.md`) will be maintained.
*   **Hidden Logic:** The existing `.OwlBot.yaml` contains subtle, conditional logic that must be audited and migrated into the new Go post-processor to ensure identical output.
*   **Symlink Handling:** The Librarian tooling's support for preserving symlinks must be verified, as several key files in the existing repository are symlinks.


A critical design decision is to forgo the existing, complex [post-processor](TODO) in favor of a new, lightweight implementation directly within the `librariangen` binary. The current post-processor is unsuitable as it depends on a full `google-cloud-go` repository checkout and makes calls to the GitHub API.

The new, self-contained post-processor runs entirely within the `/output` directory and performs the following essential tasks:
1.  **`goimports`:** Runs `goimports -w .` to format the generated code and fix imports.
2.  **Module Initialization:** Runs `go mod init` and `go mod tidy` to create a valid `go.mod` file for the newly generated code. This is a prerequisite for running other Go tools.
3.  **`version.go` Generation:** A `version.go` file is generated for each new module, containing the library version provided in the request.
4.  **Linting:** Runs `staticcheck ./...` to catch potential issues in the generated code.

*Example expected output structure after post-processing:* (TODO)
```shell
cloud.google.com/
└── go
    ├── workflows
    │   └── apiv1
    │       ├── workflows_client.go
    │       ├── workflowspb
    │       │   ├── workflows_service.pb.go
    │       │   └── ...
    │       ├── doc.go
    │       └── ...
    └── internal
        └── generated
            └── snippets
                └── workflows
                    └── apiv1
                        ├── Client
                        │   ├── AnalyzeIamPolicy
                        │   │   └── main.go
                        │   └── ...
                        └── snippet_metadata.google.cloud.workflows.v1.json
```

# Testing Plan

A multi-faceted testing strategy is required to ensure a safe and successful migration.

1.  **Manual E2E Testing:**
    *   **Phase 1 (Validation):** Manually run the `librarian` CLI to regenerate ~10 existing libraries. Verify that the process completes successfully and produces **no diff** against the current code.
    *   **Phase 2 (Onboarding):** Use a script to run the `librarian` CLI to onboard the remaining ~180 libraries, verifying that `state.yaml` is correctly populated and that the generated code also produces no diff.

2.  **Smoke Testing:** A shell script will be used to perform basic validation of the Docker image itself, ensuring that all required tools (`protoc`, `go`, etc.) and dependencies are installed correctly. This can be run manually or as part of an automated CI check.

3.  **Docker-based E2E Testing:** An automated script (`run-container-tests.sh`) will execute the full, containerized workflow for a suite of test libraries, covering important edge cases. A test is successful if:
    *   The `generate` command completes successfully.
    *   The subsequent `build` command, running against the newly generated code, passes all its tests (`go build` and `go test`).

4.  **Unit Testing:** All new Go code written for the generator and post-processor will be accompanied by a comprehensive suite of unit tests.
# Alternatives considered

1.  **Adapt Bazel-based Generation:** The primary alternative was to continue using the existing Bazel-based system. This was rejected due to its high maintenance cost, complexity, and security concerns. It also prevents Go from aligning with the standardized, cross-language Librarian pipeline.

2.  **Adapt the Old Post-Processor:** An attempt was made to run the existing `post-processor` tool inside the container. This was deemed infeasible. The tool is not designed to be portable; it requires a full `google-cloud-go` repository context (including a `.git` directory and `go.work` file) and contains logic for interacting with GitHub pull requests, none of which are available or relevant inside the generator container. The chosen approach of a new, focused post-processor is far simpler and more robust.

# Work Estimates

Please refer to the [GitHub project](TODO: link) for specific deliverables and delivery dates.


# Documentation plan

*   **`generator/README.md`:** This will be the primary technical document for the Go generator, detailing its architecture, commands, and how to run it locally. It will be kept up-to-date as new features are added.
*   **`docs/librarian_guide.md`:** This central Librarian document will be updated to include instructions and details for onboarding and generating Go libraries using the new system.
*   **This Design Doc:** This document will serve as the persistent, high-level design reference.

All documentation will be updated and made available before the first library is fully migrated to the new system.
# Launch plans

The migration to the new generator will be a gradual process, managed by the central Librarian tool's configuration (`state.yaml`).

*   **Visible changes:** The code generation process for Go libraries will become faster and more transparent. The underlying CI/CD jobs will be simplified significantly.
*   **Impact on production:** There will be no direct impact on production services. The generated code will be functionally identical to the code produced by the old system.
*   **New servers:** No new infrastructure is required; this system leverages the existing Librarian pipeline.
*   **Supportability:** Long-term support will be greatly simplified. The generator is a standard Go application, making it easier for any Go developer to contribute, as opposed to the specialized Bazel knowledge required by the old system.
*   **Timeline:** The rollout will proceed on a library-by-library basis over the course of a few weeks, starting with a few pilot libraries to validate the process before expanding to all Go clients.
# Risks

*   **Hidden Logic in Configuration Files:** The `.OwlBot.yaml` and underlying Bazel rules may contain subtle, undocumented logic. Migrating this correctly is critical for backward compatibility.
    *   **Mitigation:** Before full migration, perform a thorough audit of the existing configuration to identify and categorize all implicit logic. Allocate specific time for this investigation.
*   **Updating Shared Files:** The migration must correctly handle updates to files that are shared across multiple modules (e.g., `internal/.repo-metadata-full.json`).
    *   **Mitigation:** The post-processor must be explicitly designed to handle these shared files, potentially by having a separate processing step that runs after all individual libraries are generated.
*   **Configuration Edge Cases:** The logic for parsing `BUILD.bazel` files in `googleapis` might not account for all possible configurations or edge cases.
    *   **Mitigation:** Test the generator against a diverse set of existing Go libraries before the migration. Add robust error handling for unexpected `BUILD.bazel` structures.
*   **Post-Processing Discrepancies:** The new, streamlined post-processor might not perform all the subtle modifications that the old system did, potentially leading to minor differences in generated code.
    *   **Mitigation:** Before migrating each library, perform a `diff` between the output of the old and new systems. Any significant differences will be investigated and the post-processor will be adjusted as needed.
*   **Tool Versioning:** The generator's `Dockerfile` pins versions for `protoc` and its Go plugins. These dependencies can become stale.
    *   **Mitigation:** Implement automated dependency scanning (e.g., RenovateBot) for the `Dockerfile` to create pull requests whenever new tool versions are released.
